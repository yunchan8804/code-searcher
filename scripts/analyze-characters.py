#!/usr/bin/env python3
"""
Character Database Analyzer
===========================
characters.json íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í˜„ì¬ ìƒíƒœì™€ ë¹ ì§„ ë¬¸ìë¥¼ íŒŒì•…í•˜ëŠ” ë„êµ¬

ì‚¬ìš©ë²•:
    python analyze-characters.py [--summary] [--category <name>] [--gaps] [--unicode-blocks]

ì˜µì…˜:
    --summary        ì „ì²´ ìš”ì•½ (ê¸°ë³¸ê°’)
    --category NAME  íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë¶„ì„
    --gaps           ë¹ ì§„ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ë¶„ì„
    --unicode-blocks ìœ ë‹ˆì½”ë“œ ë¸”ë¡ë³„ ì»¤ë²„ë¦¬ì§€
    --json           JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
"""

import json
import sys
import argparse
import io

# Windows í„°ë¯¸ë„ UTF-8 ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
from pathlib import Path
from collections import defaultdict

# ìœ ë‹ˆì½”ë“œ ë¸”ë¡ ì •ì˜ (ì£¼ìš” ë¸”ë¡ë§Œ)
UNICODE_BLOCKS = {
    "Basic Latin": (0x0000, 0x007F),
    "Latin-1 Supplement": (0x0080, 0x00FF),
    "General Punctuation": (0x2000, 0x206F),
    "Superscripts and Subscripts": (0x2070, 0x209F),
    "Currency Symbols": (0x20A0, 0x20CF),
    "Letterlike Symbols": (0x2100, 0x214F),
    "Number Forms": (0x2150, 0x218F),
    "Arrows": (0x2190, 0x21FF),
    "Mathematical Operators": (0x2200, 0x22FF),
    "Miscellaneous Technical": (0x2300, 0x23FF),
    "Enclosed Alphanumerics": (0x2460, 0x24FF),
    "Box Drawing": (0x2500, 0x257F),
    "Block Elements": (0x2580, 0x259F),
    "Geometric Shapes": (0x25A0, 0x25FF),
    "Miscellaneous Symbols": (0x2600, 0x26FF),
    "Dingbats": (0x2700, 0x27BF),
    "Supplemental Arrows-A": (0x27F0, 0x27FF),
    "Supplemental Arrows-B": (0x2900, 0x297F),
    "Miscellaneous Symbols and Arrows": (0x2B00, 0x2BFF),
    "Greek and Coptic": (0x0370, 0x03FF),
    "Emoticons": (0x1F600, 0x1F64F),
    "Miscellaneous Symbols and Pictographs": (0x1F300, 0x1F5FF),
    "Transport and Map Symbols": (0x1F680, 0x1F6FF),
    "Supplemental Symbols and Pictographs": (0x1F900, 0x1F9FF),
    "Symbols and Pictographs Extended-A": (0x1FA00, 0x1FA6F),
    "Regional Indicator Symbols": (0x1F1E0, 0x1F1FF),
}

# ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆìƒ ìœ ë‹ˆì½”ë“œ ë²”ìœ„
CATEGORY_UNICODE_RANGES = {
    "arrow": [(0x2190, 0x21FF), (0x27F0, 0x27FF), (0x2900, 0x297F)],
    "math": [(0x2200, 0x22FF), (0x2A00, 0x2AFF)],
    "greek": [(0x0370, 0x03FF)],
    "line": [(0x2500, 0x257F), (0x2580, 0x259F)],
    "geometric": [(0x25A0, 0x25FF), (0x2B00, 0x2B4F)],
    "circled": [(0x2460, 0x24FF), (0x3200, 0x32FF)],
    "superscript": [(0x2070, 0x209F)],
    "currency": [(0x20A0, 0x20CF)],
    "punctuation": [(0x2000, 0x206F)],
    "music": [(0x2669, 0x266F), (0x1F3B5, 0x1F3BC)],
    "emoji": [(0x1F600, 0x1F64F), (0x1F300, 0x1F5FF), (0x1F680, 0x1F6FF)],
    "animal": [(0x1F400, 0x1F43F), (0x1F980, 0x1F9AF)],
    "food": [(0x1F345, 0x1F37F), (0x1F950, 0x1F96F)],
    "flag": [(0x1F1E0, 0x1F1FF), (0x1F3C1, 0x1F3F4)],
    "star": [(0x2721, 0x2739), (0x2605, 0x2606)],
    "heart": [(0x2661, 0x2665), (0x2763, 0x2765), (0x1F493, 0x1F49F)],
    "weather": [(0x2600, 0x2602), (0x2614, 0x2614), (0x26C4, 0x26C8), (0x1F300, 0x1F32D)],
    "check": [(0x2610, 0x2612), (0x2713, 0x2718), (0x2705, 0x2705)],
    "zodiac": [(0x2648, 0x2653)],
    "roman": [(0x2160, 0x216F), (0x2170, 0x217F)],
    "hand": [(0x261A, 0x261F), (0x1F446, 0x1F450), (0x1F91A, 0x1F91F)],
    "face": [(0x1F600, 0x1F64F)],
    "object": [(0x1F451, 0x1F4FF), (0x1F6A0, 0x1F6FF)],
    "game": [(0x2654, 0x265F), (0x2660, 0x2667), (0x1F0A0, 0x1F0FF)],
    "bracket": [(0x0028, 0x0029), (0x005B, 0x005D), (0x007B, 0x007D), (0x2768, 0x2775), (0x27E6, 0x27EF)],
}


def load_characters(filepath):
    """characters.json íŒŒì¼ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('characters', data) if isinstance(data, dict) else data


def get_codepoint_value(codepoint_str):
    """ìœ ë‹ˆì½”ë“œ ì½”ë“œí¬ì¸íŠ¸ ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜"""
    # "U+1F600" ë˜ëŠ” "U+1F1F0 U+1F1F7" í˜•íƒœ
    parts = codepoint_str.split()
    if parts:
        return int(parts[0].replace("U+", ""), 16)
    return 0


def analyze_categories(characters):
    """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„"""
    categories = defaultdict(list)
    for char in characters:
        cat = char.get('category', 'unknown')
        categories[cat].append(char)
    return categories


def get_category_summary(categories):
    """ì¹´í…Œê³ ë¦¬ ìš”ì•½ ìƒì„±"""
    summary = []
    for cat, chars in sorted(categories.items(), key=lambda x: -len(x[1])):
        codepoints = [get_codepoint_value(c.get('codepoint', '')) for c in chars]
        min_cp = min(codepoints) if codepoints else 0
        max_cp = max(codepoints) if codepoints else 0
        summary.append({
            'category': cat,
            'count': len(chars),
            'codepoint_range': f"U+{min_cp:04X} - U+{max_cp:04X}",
            'sample': chars[0]['char'] if chars else ''
        })
    return summary


def analyze_unicode_coverage(characters):
    """ìœ ë‹ˆì½”ë“œ ë¸”ë¡ë³„ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
    codepoints = set()
    for char in characters:
        cp = get_codepoint_value(char.get('codepoint', ''))
        if cp:
            codepoints.add(cp)

    coverage = []
    for block_name, (start, end) in UNICODE_BLOCKS.items():
        block_size = end - start + 1
        covered = len([cp for cp in codepoints if start <= cp <= end])
        if covered > 0:
            coverage.append({
                'block': block_name,
                'range': f"U+{start:04X}-U+{end:04X}",
                'total': block_size,
                'covered': covered,
                'percentage': round(covered / block_size * 100, 1)
            })

    return sorted(coverage, key=lambda x: -x['covered'])


def find_gaps(characters, category=None):
    """ë¹ ì§„ ìœ ë‹ˆì½”ë“œ ë²”ìœ„ ì°¾ê¸°"""
    if category and category in CATEGORY_UNICODE_RANGES:
        ranges = CATEGORY_UNICODE_RANGES[category]
    else:
        ranges = list(UNICODE_BLOCKS.values())

    # í˜„ì¬ ìˆëŠ” ì½”ë“œí¬ì¸íŠ¸
    existing = set()
    for char in characters:
        if category is None or char.get('category') == category:
            cp = get_codepoint_value(char.get('codepoint', ''))
            if cp:
                existing.add(cp)

    gaps = []
    for start, end in ranges:
        missing = []
        for cp in range(start, end + 1):
            if cp not in existing:
                # ìœ íš¨í•œ ìœ ë‹ˆì½”ë“œ ë¬¸ìì¸ì§€ í™•ì¸ (ê°„ë‹¨íˆ)
                try:
                    char = chr(cp)
                    if char.isprintable() or cp >= 0x1F300:
                        missing.append(cp)
                except:
                    pass

        if missing and len(missing) < (end - start + 1):  # ì™„ì „íˆ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
            gaps.append({
                'range': f"U+{start:04X}-U+{end:04X}",
                'missing_count': len(missing),
                'sample_missing': [f"U+{cp:04X}" for cp in missing[:5]]
            })

    return gaps


def print_summary(characters):
    """ì „ì²´ ìš”ì•½ ì¶œë ¥"""
    categories = analyze_categories(characters)
    summary = get_category_summary(categories)

    print("=" * 60)
    print("ğŸ“Š CHARACTER DATABASE SUMMARY")
    print("=" * 60)
    print(f"\nì´ ë¬¸ì ìˆ˜: {len(characters)}ê°œ")
    print(f"ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}ê°œ")
    print()
    print("-" * 60)
    print(f"{'ì¹´í…Œê³ ë¦¬':<15} {'ê°œìˆ˜':>6} {'ìƒ˜í”Œ':>6} {'ì½”ë“œí¬ì¸íŠ¸ ë²”ìœ„':<25}")
    print("-" * 60)

    for item in summary:
        print(f"{item['category']:<15} {item['count']:>6} {item['sample']:>6} {item['codepoint_range']:<25}")

    print("-" * 60)
    print(f"{'í•©ê³„':<15} {len(characters):>6}")
    print()


def print_unicode_coverage(characters):
    """ìœ ë‹ˆì½”ë“œ ë¸”ë¡ ì»¤ë²„ë¦¬ì§€ ì¶œë ¥"""
    coverage = analyze_unicode_coverage(characters)

    print("=" * 60)
    print("ğŸ“¦ UNICODE BLOCK COVERAGE")
    print("=" * 60)
    print()
    print(f"{'ë¸”ë¡ëª…':<35} {'ì»¤ë²„':>6} {'ì „ì²´':>6} {'%':>6}")
    print("-" * 60)

    for item in coverage:
        bar = "â–ˆ" * int(item['percentage'] / 10)
        print(f"{item['block']:<35} {item['covered']:>6} {item['total']:>6} {item['percentage']:>5}% {bar}")

    print()


def print_category_detail(characters, category):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìƒì„¸ ì¶œë ¥"""
    cats = analyze_categories(characters)
    if category not in cats:
        print(f"ì¹´í…Œê³ ë¦¬ '{category}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {', '.join(sorted(cats.keys()))}")
        return

    chars = cats[category]
    print("=" * 60)
    print(f"ğŸ“‚ CATEGORY: {category}")
    print("=" * 60)
    print(f"\nì´ {len(chars)}ê°œ ë¬¸ì\n")

    # ì½”ë“œí¬ì¸íŠ¸ ìˆœ ì •ë ¬
    chars_sorted = sorted(chars, key=lambda x: get_codepoint_value(x.get('codepoint', '')))

    print(f"{'ë¬¸ì':>4} {'ì½”ë“œí¬ì¸íŠ¸':<12} {'ì´ë¦„':<30} {'íƒœê·¸(í•œê¸€)':<20}")
    print("-" * 70)

    for char in chars_sorted[:50]:  # ì²˜ìŒ 50ê°œë§Œ
        tags = ', '.join(char.get('tags_ko', [])[:2])
        print(f"{char['char']:>4} {char.get('codepoint', ''):<12} {char.get('name', '')[:28]:<30} {tags:<20}")

    if len(chars) > 50:
        print(f"\n... ì™¸ {len(chars) - 50}ê°œ ë” ìˆìŒ")

    # ê°­ ë¶„ì„
    print("\n" + "-" * 60)
    print("ë¹ ì§„ ë¬¸ì ë¶„ì„:")
    gaps = find_gaps(characters, category)
    if gaps:
        for gap in gaps[:3]:
            print(f"  - {gap['range']}: {gap['missing_count']}ê°œ ë¹ ì§")
            print(f"    ì˜ˆì‹œ: {', '.join(gap['sample_missing'])}")
    else:
        print("  ë¶„ì„í•  ë²”ìœ„ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print()


def print_gaps(characters):
    """ë¹ ì§„ ë²”ìœ„ ì¶œë ¥"""
    print("=" * 60)
    print("ğŸ” GAP ANALYSIS (ë¹ ì§„ ë¬¸ì ë²”ìœ„)")
    print("=" * 60)
    print()

    for category in sorted(CATEGORY_UNICODE_RANGES.keys()):
        gaps = find_gaps(characters, category)
        if gaps:
            print(f"ğŸ“ {category}:")
            for gap in gaps:
                print(f"   {gap['range']}: {gap['missing_count']}ê°œ ë¹ ì§")
            print()


def main():
    parser = argparse.ArgumentParser(description='Character Database Analyzer')
    parser.add_argument('--summary', action='store_true', help='ì „ì²´ ìš”ì•½')
    parser.add_argument('--category', type=str, help='íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìƒì„¸')
    parser.add_argument('--gaps', action='store_true', help='ë¹ ì§„ ë²”ìœ„ ë¶„ì„')
    parser.add_argument('--unicode-blocks', action='store_true', help='ìœ ë‹ˆì½”ë“œ ë¸”ë¡ ì»¤ë²„ë¦¬ì§€')
    parser.add_argument('--json', action='store_true', help='JSON ì¶œë ¥')
    parser.add_argument('--file', type=str, help='characters.json ê²½ë¡œ')

    args = parser.parse_args()

    # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    if args.file:
        filepath = Path(args.file)
    else:
        # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì°¾ê¸°
        script_dir = Path(__file__).parent
        filepath = script_dir.parent / 'src' / 'UnicodeSearcher' / 'Data' / 'characters.json'

    if not filepath.exists():
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        sys.exit(1)

    characters = load_characters(filepath)

    # ê¸°ë³¸ê°’: summary
    if not any([args.summary, args.category, args.gaps, args.unicode_blocks]):
        args.summary = True

    if args.json:
        result = {
            'total': len(characters),
            'categories': get_category_summary(analyze_categories(characters)),
            'unicode_coverage': analyze_unicode_coverage(characters)
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        if args.summary:
            print_summary(characters)
        if args.unicode_blocks:
            print_unicode_coverage(characters)
        if args.category:
            print_category_detail(characters, args.category)
        if args.gaps:
            print_gaps(characters)


if __name__ == '__main__':
    main()
